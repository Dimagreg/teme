{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset is in the population_profits.txt file\n",
    "# we can use numpy.loadtxt to load such basic files\n",
    "dataset = numpy.loadtxt(\"population_profits.txt\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy.loadtxt produces a numpy array\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.1101, 17.592 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the numpy array itself contains numpy arrays\n",
    "# where each such array represents an example from the dataset \n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataset: must inherit from Dataset\n",
    "class PopulationProfitDataset(Dataset):\n",
    "    # constructor: usually this is where we place the actual data in the Dataset object\n",
    "    def __init__(self, numpy_array):\n",
    "        self.numpy_array = numpy_array\n",
    "\n",
    "    # the __getitem__ magic method is what is actually called behind the scenes in Python\n",
    "    # when we use indexing. It's role is to return the element in position index from our dataset\n",
    "    # Usually we return 2 tensors from this method: one with the inputs to our model\n",
    "    # and one with the outputs\n",
    "    # generally we write __getitem__ so that its output will make our lives easier when\n",
    "    # using it\n",
    "    def __getitem__(self, index):\n",
    "        example = self.numpy_array[index]\n",
    "        feature = example[0]\n",
    "        target = example[1]\n",
    "        return torch.Tensor([feature]), torch.Tensor([target])\n",
    "    \n",
    "    # magic method the gets called behind the scenes when we want to find the length of something\n",
    "    # in our case, returns the size of the dataset\n",
    "    # note: numpy_array is a 90 x 2 array, because we have 90 examples in our dataset and each example\n",
    "    # is described by a population and a profit value. len(numpy_array) returns 90, which is the first\n",
    "    # 'dimension' of the numpy_array\n",
    "    def __len__(self):\n",
    "        return len(self.numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we instantiate the dataset that we defined earlier, passing it the numpy array\n",
    "torch_dataset = PopulationProfitDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.7107]), tensor([3.2522]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is equivalent to torch_dataset.__getitem__(10), which we defined earlier\n",
    "# it returns 2 tensors, one containing the population (model input)\n",
    "# and the other containing the population (model output)\n",
    "torch_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is equivalent to torch_dataset.__len__(). When we defined the __len__ method earlier,\n",
    "# we essentially told Python that when we call `len` on the dataset object, we want it to \n",
    "# call the `len` method of the numpy_array that our dataset actually contains\n",
    "len(torch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to use our hardware to the max when training a model\n",
    "# so we usually try to pass as many examples from the dataset through\n",
    "# the model as we possible can.\n",
    "# A dataloader takes in a dataset and gives us multiple examples from it at once\n",
    "# in this case, when asking the dataloader for more data, it will give us 10 entries\n",
    "# from the dataset\n",
    "torch_loader = DataLoader(torch_dataset, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataloader is a python generator\n",
    "# at this point there is nothing in the dataloader\n",
    "# it hasn't queried the dataset for data yet\n",
    "# and it hasn't split it into minibatches (group of 10 entries, as mentioned above)\n",
    "\n",
    "# when we request something from the dataloader (or from a python generator, in general)\n",
    "# it generates (hence its name) the data right then and there\n",
    "# this is useful when we have loads and loads of data\n",
    "# for example, if we train on a dataset of a million images, we can't load them all into\n",
    "# memory at once. In such a case, we would write our `__getitem__` method \n",
    "# so that it loads image number `index` from disk and returns it\n",
    "\n",
    "# in that situation, when we would ask the dataloader for a minibatch\n",
    "# it would call the __getitem__ method of the dataset n times\n",
    "# load the n images right then and there, and return them to us\n",
    "# in this way we can work with huge datasets even though we can't\n",
    "# store the whole dataset in memory at once\n",
    "\n",
    "# dataloaders and generators remember their history\n",
    "# so next time we call on the dataloader / generator for data\n",
    "# it will know where it last left off and will continue by giving us the next batch\n",
    "for feature_batch, target_batch in torch_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.1101],\n",
       "        [5.5277],\n",
       "        [8.5186],\n",
       "        [7.0032],\n",
       "        [5.8598],\n",
       "        [8.3829],\n",
       "        [7.4764],\n",
       "        [8.5781],\n",
       "        [6.4862],\n",
       "        [5.0546]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# above we used break in the for loop, which means we stop after the dataloader gives us the first batch of data\n",
    "# we can see a tensor of shape 10 x 1, meaning we received 10 examples from the dataset, each of them containing once value\n",
    "# this looks equivalent to a tensor of shape 10, and we could store this data in such a tensor, but\n",
    "# we usually would have more than one value for each example, in which case a 2D tensor would be necessary\n",
    "# so we make the decision to always use 2D tensors in this situation\n",
    "# notice that the inner tensors in the batch (the [6.1101], the [5.5277], etc) are of the same shape\n",
    "# as the tensors that __getitem__ returns, because the dataloader used that method to obtain \n",
    "# the data that it gave us\n",
    "feature_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17.5920],\n",
       "        [ 9.1302],\n",
       "        [13.6620],\n",
       "        [11.8540],\n",
       "        [ 6.8233],\n",
       "        [11.8860],\n",
       "        [ 4.3483],\n",
       "        [12.0000],\n",
       "        [ 6.5987],\n",
       "        [ 3.8166]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we instantiate a linear regression model. The 2 parameters of nn.Linear represent the number of inputs to the model,\n",
    "# respectively how many outputs we expect it to have: in our case, we have one input (the population) \n",
    "# and one output (the profit)\n",
    "# this model implements the following equation: profit = a * population + b\n",
    "model = nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2469]], requires_grad=True)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch stores the coefficients that we multiply inputs by in nn.Linear(_, _).weight\n",
    "# in our case, this is a single parameter, representing a\n",
    "# notice that the tensor has requires_grad = True: we intend to modify it's value later \n",
    "# through gradient/derivative based optimization\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3258], requires_grad=True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch stores the free term (bias term, b in our equation) in nn.Linear(_, _).bias\n",
    "# as above, this tensor has requires_grad = True\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1830], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use the model, we give a tensor containing one single value as input\n",
    "# this value represents a population in the problem we're trying to solve\n",
    "# the model outputs another value, which we interpret as profit\n",
    "model(torch.Tensor([6.11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model has random values for its parameters, so we expect it to be quite terrible\n",
    "# we need an exact measurement of how terrible it is at the task we're trying to use it for\n",
    "# (give us the profit when we give it a population)\n",
    "# for problems like this one, we can use the mean squared error loss\n",
    "# as with most things in Pytorch, we first need to instantiate the loss function\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9050.)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use the loss function, we pass it 2 tensors\n",
    "# first tensor represents the output of our model (which we expect to be useless currently)\n",
    "# and the second tensor represents the correct out, i.e. what we would have liked the first\n",
    "# tensor to be equal to \n",
    "# MSELoss will now subtract the corresponding values in the 2 tensors\n",
    "# followed by raising the result to the second power\n",
    "# and then computing the mean of all of these error terms\n",
    "# good model: small loss, bad model: big loss\n",
    "loss_function(torch.Tensor([100, 120]), torch.Tensor([10, 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's no point in knowing how bad the model is if we can't improve it\n",
    "# to improve the model, we use what is called an optimizer\n",
    "# the optimizer is going to update our model's parameters\n",
    "# so it must know what those parameters are\n",
    "# that is why we pass it model.parameters() as the first argument\n",
    "# the optimizer will change the model's parameters by some step size\n",
    "# it has isn't own logic for choosing the step size \n",
    "# (for SGD, it uses the opposite of the gradient that it finds stored in the model parameters)\n",
    "# but we can control that step size to some extent by multiplying it with a value that we choose\n",
    "# we usually go for small values at first, and can then increase them if we notice our model improves too slow\n",
    "# or decrease them if we end up with a model that doesn't learn anything\n",
    "optimizer = SGD(model.parameters(), lr = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2469]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.3258], requires_grad=True)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we look the model.parameters(), we notice it is a generator\n",
    "# a generator doesn't contain any values yet, but it knows how to obtain them\n",
    "# for our purpose (to see what's in model.parameters, that is) we can ask the generator\n",
    "# to give us all of its values by casting it to a list\n",
    "# notice in the following 2 cells that model.weight and model.bias can be found in the parameters list\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2469]], requires_grad=True)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3258], requires_grad=True)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.3659, grad_fn=<DivBackward0>)\n",
      "tensor(17.8078, grad_fn=<DivBackward0>)\n",
      "tensor(17.4761, grad_fn=<DivBackward0>)\n",
      "tensor(17.1786, grad_fn=<DivBackward0>)\n",
      "tensor(16.8935, grad_fn=<DivBackward0>)\n",
      "tensor(16.6201, grad_fn=<DivBackward0>)\n",
      "tensor(16.3580, grad_fn=<DivBackward0>)\n",
      "tensor(16.1067, grad_fn=<DivBackward0>)\n",
      "tensor(15.8658, grad_fn=<DivBackward0>)\n",
      "tensor(15.6348, grad_fn=<DivBackward0>)\n",
      "tensor(15.4132, grad_fn=<DivBackward0>)\n",
      "tensor(15.2008, grad_fn=<DivBackward0>)\n",
      "tensor(14.9971, grad_fn=<DivBackward0>)\n",
      "tensor(14.8018, grad_fn=<DivBackward0>)\n",
      "tensor(14.6144, grad_fn=<DivBackward0>)\n",
      "tensor(14.4347, grad_fn=<DivBackward0>)\n",
      "tensor(14.2624, grad_fn=<DivBackward0>)\n",
      "tensor(14.0971, grad_fn=<DivBackward0>)\n",
      "tensor(13.9385, grad_fn=<DivBackward0>)\n",
      "tensor(13.7864, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we are now ready to train the model\n",
    "# we will run 20 training iterations (in ML, an iteration is called an epoch)\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0\n",
    "    # we query the dataloader for mini-batches\n",
    "    for features_batch, targets_batch in torch_loader:\n",
    "        # for each minibatch, we run it through the model\n",
    "        # obtaining a prediction\n",
    "        # we would like the prediction to be equal to targets_batch\n",
    "        # because features_batch represents populations and\n",
    "        # targets_batch represents profit and we would like our model to\n",
    "        # be good at predicting profit from population \n",
    "        predicted_targets_batch = model(features_batch)\n",
    "        # we compute the loss of the model using MSELoss\n",
    "        loss = loss_function(predicted_targets_batch, targets_batch)\n",
    "        # we reset the gradient value to 0\n",
    "        # otherwise PyTorch would keep adding the new gradient value over the old one\n",
    "        optimizer.zero_grad()\n",
    "        # at this point we ran some inputs through the model\n",
    "        # and computed the loss\n",
    "        # behind the scenes, PyTorch has constructed a graph of everything we have done\n",
    "        # this way, it knows how to compute the gradient of the loss function with respect to\n",
    "        # the parameters (model.weight and model.bias) of the model\n",
    "        # we compute those gradients using loss.backward()\n",
    "        loss.backward()\n",
    "        # finally, we take an optimization step: we change the value of the model parameters\n",
    "        # in such a way that our model should improve\n",
    "        optimizer.step()\n",
    "        # we also would like to compute a loss for the whole dataset\n",
    "        # remember that MSELoss computes a mean of the error when we give more than one prediction - correct output pair\n",
    "        # so we just add all losses up here\n",
    "        # and will later on divide the epoch_loss by how many minibatches we have\n",
    "        epoch_loss += loss\n",
    "\n",
    "    epoch_loss = epoch_loss / (len(torch_dataset) / 10)\n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.7316]], requires_grad=True)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1.0277], requires_grad=True)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.1415], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use the model (that is now trained) we simply give it some value of the population\n",
    "# and it will spit out a value for the profit\n",
    "# EXTRA: notice the output tensor has grad_fn=something\n",
    "# This means that pytorch has constructed a graph of everything we've done so far\n",
    "# so that it could compute gradients if we ask it to. Gradients are useful when trying \n",
    "# to train a model, but here we just want to use it. Use Google to figure out\n",
    "# how to prevent pytorch from computing gradients, and see what the output looks like\n",
    "# when PyTorch doesn't do that\n",
    "model(torch.Tensor([18]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
