{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, cache_dir=os.path.join('..', 'data')):\n",
    "    \"\"\"Download a file, return the local filename.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "        return fname\n",
    "    print(f'Downloading {fname} from {url}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_time_machine():\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    with open(download('http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):  # Token frequencies\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # Since each text line in the time machine dataset is not necessarily a\n",
    "    # sentence or a paragraph, flatten all the text lines into a single list\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    \"\"\"Generate a mini-batch of subsequences using sequential partitioning.\"\"\"\n",
    "    # Start with a random offset to partition a sequence\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:\n",
    "    \"\"\"An iterator to load sequence data.\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, max_tokens):\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return seq_data_iter_sequential(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_time_machine(batch_size, num_steps, max_tokens=10000):\n",
    "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificari: `batch_size` devine 30, `num_steps` devine 10.\n",
    "\n",
    "Number of steps este numarul de litere (respectiv de cuvinte, daca am antrena pe cuvinte) ce il dam intr-un moment modelului"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 30, 10\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_hiddens` este modificat la `32`. Acesta reprezinta dimensiunea vectorului de stare al modelului (`256` in discutiile de la laborator).\n",
    "\n",
    "RNN-ul primeste si parametrul `num_layers` pentru a avea 2 straturi de RNN. Asta inseamna ca exista un strat ca cel discutat la laborator, cu o stare interna ce se actualizeaza pentru fiecare litera din input (aici antrenam pe litere, si nu pe cuvinte), respectiv un strat care functioneaza identic, doar ca nu are ca input literele, ci output-ul primului strat. Outputul primului strat, si al oricarui strat de RNN este lista de valori ce le-a avut starea sa interna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 32\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens, num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return  torch.zeros((self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                 device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states\n",
    "            return (torch.zeros((\n",
    "                self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds, net, vocab, device):\n",
    "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, optimizer, device):\n",
    "    \"\"\"Train a net within one epoch.\"\"\"\n",
    "    state = None\n",
    "    # Sum of training loss, no. of tokens\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    for X, Y in train_iter:\n",
    "        if state is None:\n",
    "            # Initialize `state` when it is the first iteration\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if not isinstance(state, tuple):\n",
    "                # `state` is a tensor for `nn.GRU`\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state` is a tuple of tensors for `nn.LSTM`\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        optimizer.step()\n",
    "        total_loss += float(l * y.numel())\n",
    "        total_tokens += y.numel()\n",
    "    return math.exp(total_loss / total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, vocab, lr, num_epochs, device):\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    perplexities = []\n",
    "    # Initialize\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl = train_epoch(\n",
    "            net, train_iter, loss, optimizer, device)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller', 50, net, vocab, device))\n",
    "            perplexities.append(ppl)\n",
    "    print(f'perplexity {ppl:.1f}, device {str(device)}')\n",
    "    print(predict('time traveller', 50, net, vocab, device))\n",
    "    print(predict('traveller', 50, net, vocab, device))\n",
    "\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificari: `num_epochs` devine 200, learning rate-ul devine `1.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller the the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller a dome this the gere the mat the mat a dome this \n",
      "time traveller a moment we the the time the time the time the ti\n",
      "time traveller a mom the simensions of specty in a mom the time \n",
      "time traveller a lant and we the pes the pes the pes the pes the\n",
      "time traveller and the sat and the sat and the sat and the sat a\n",
      "time traveller and the there and the time travely mat se time tr\n",
      "time traveller a man a dimension and the said a man a dimension \n",
      "time traveller a d the time the time the time the time the time \n",
      "time traveller said the thing the prent on a mon the prent on a \n",
      "time traveller a manted a mont a sable the prolly and the prolly\n",
      "time traveller and of the thile s of space the thile s of space \n",
      "time traveller showe stin s the pe our and the pe our and the pe\n",
      "time traveller a mon a mont the thich and the thile the thich an\n",
      "time traveller shout a sags the thing the thing the thing the th\n",
      "time traveller a cont a cont a cont a cont a cont a cont a cont \n",
      "time traveller a man i why and for a gome dimensions the time tr\n",
      "time traveller a sags and the time the the the licht a said the \n",
      "time traveller the thich a said the thich a said the thich a sai\n",
      "perplexity 5.5, device cuda:0\n",
      "time traveller the thich a said the thich a said the thich a sai\n",
      "traveller the thich a said the thich a said the thich a sai\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 200, 1.5\n",
    "perplexities = train(net, train_iter, vocab, lr, num_epochs, device) #1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afisam perplexity-ul final (care nu este decat $e^\\text{media tuturor termenilor de cross entropy loss}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.484856475861382"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folosim `predict` pentru a genera 20 de caractere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traveller the thich a said th\n"
     ]
    }
   ],
   "source": [
    "print(predict('traveller', 20, net, vocab, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
